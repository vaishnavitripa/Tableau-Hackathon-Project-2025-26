{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f5852eb",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# PRODUCT DATA Analysis\n",
    "# =========================\n",
    "\n",
    "## Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "file_path = r\"/Users/vini/Downloads/farfetch hackathon data/fashion_sales.xlsm\"\n",
    "\n",
    "## load product dataset\n",
    "df_products = pd.read_excel(file_path, sheet_name=\"Product_Master\")\n",
    "\n",
    "## load mock dataset\n",
    "df_mock = pd.read_excel(file_path, sheet_name=\"MockData\")\n",
    "\n",
    "## Quick check\n",
    "print(\"Products dataset:\")\n",
    "print(df_products.head())\n",
    "print(df_products.info())\n",
    "\n",
    "print(\"\\nMock dataset:\")\n",
    "print(df_mock.head())\n",
    "print(df_mock.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add14384",
   "metadata": {},
   "source": [
    "# distribution of price range per brand (static attribute)\n",
    "\n",
    "## Defensive checks / convert Price to numeric\n",
    "print(\"Before conversion:\", df_products['Price'].dtype)\n",
    "df_products['Price'] = pd.to_numeric(df_products['Price'], errors='coerce')   # convert; invalid -> NaN\n",
    "print(\"After conversion:\", df_products['Price'].dtype)\n",
    "print(\"Price nulls:\", df_products['Price'].isna().sum())\n",
    "print(df_products['Price'].describe())\n",
    "\n",
    "## If there are NaNs created by conversion, you can decide to drop or fill (here we drop for safety)\n",
    "df_products = df_products.dropna(subset=['Price']).copy()\n",
    "\n",
    "## Define broad price bins (use np.inf for open top)\n",
    "price_bins = [0, 500, 2000, 5000, np.inf]   # edges -> 4 bins\n",
    "price_labels = ['<500', '500-2000', '2000-5000', '5000+']\n",
    "\n",
    "## Validate labels length\n",
    "assert len(price_labels) == len(price_bins) - 1, \"Labels length must equal len(bins)-1\"\n",
    "\n",
    "## Create the categorical column\n",
    "df_products['BroadPriceRange'] = pd.cut(\n",
    "    df_products['Price'],\n",
    "    bins=price_bins,\n",
    "    labels=price_labels,\n",
    "    include_lowest=True,   # include price == 0 in first bin\n",
    "    right=False            # left-inclusive, right-exclusive intervals [a, b)\n",
    ")\n",
    "\n",
    "## Bar plot (small, readable)\n",
    "plt.figure(figsize=(6,4))\n",
    "brands_per_range.plot(kind='bar')\n",
    "plt.title(\"Unique Brands per Broad Price Range (Products)\")\n",
    "plt.ylabel(\"Number of unique brands\")\n",
    "plt.xlabel(\"Price Range\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "## Optional: print the actual brand lists for each range (useful for report)\n",
    "for pr in price_labels:\n",
    "    brands_in_range = sorted(df_products.loc[df_products['BroadPriceRange'] == pr, 'Brand'].unique())\n",
    "    print(f\"\\nPrice Range {pr} ‚Üí {len(brands_in_range)} brands\")\n",
    "    print(brands_in_range[:50])   # show first 50 to avoid huge output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Loop through each price range\n",
    "for price_range in df_products['BroadPriceRange'].unique():\n",
    "    # Filter products in this range\n",
    "    subset = df_products[df_products['BroadPriceRange'] == price_range]\n",
    "\n",
    "    # Count products per brand and get top 10\n",
    "    top_brands = subset['Brand'].value_counts().head(10)\n",
    "\n",
    "    if not top_brands.empty:  # Only plot if brands exist\n",
    "        plt.figure(figsize=(10,5))\n",
    "        top_brands.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "        plt.title(f\"Top 10 Brands in Price Range: {price_range}\")\n",
    "        plt.xlabel(\"Brand\")\n",
    "        plt.ylabel(\"Number of Products\")\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940faff0",
   "metadata": {},
   "source": [
    "## Distribution of fabrics, finishes, prints\n",
    "\n",
    "\n",
    "### ---- FABRIC Distribution ----\n",
    "fabric_counts = df_products['Fabric'].value_counts().head(15)  # top 15 only for clarity\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(y=fabric_counts.index, x=fabric_counts.values, palette=\"viridis\")\n",
    "plt.title(\"Top 15 Fabrics in Products Dataset\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Fabric\")\n",
    "plt.show()\n",
    "\n",
    "### ---- FINISH Distribution ----\n",
    "finish_counts = df_products['Finish'].value_counts().head(15)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(y=finish_counts.index, x=finish_counts.values, palette=\"magma\")\n",
    "plt.title(\"Top 15 Finishes in Products Dataset\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Finish\")\n",
    "plt.show()\n",
    "\n",
    "### ---- PRINT Distribution ----\n",
    "print_counts = df_products['Print'].value_counts().head(15)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(y=print_counts.index, x=print_counts.values, palette=\"cubehelix\")\n",
    "plt.title(\"Top 15 Prints in Products Dataset\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Print\")\n",
    "plt.show()\n",
    "\n",
    "### count of styles per category\n",
    "style_per_category = df_products.groupby(\"Category\")['Style'].nunique().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=style_per_category.values, y=style_per_category.index, palette=\"coolwarm\")\n",
    "plt.title(\"Unique Styles per Category\")\n",
    "plt.xlabel(\"Number of Unique Styles\")\n",
    "plt.ylabel(\"Category\")\n",
    "plt.show()\n",
    "\n",
    "### outlier detection in price\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(x=df_products['Price'], palette=\"Set2\")\n",
    "plt.title(\"Outlier Detection in Product Prices\")\n",
    "plt.xlabel(\"Price\")\n",
    "plt.show()\n",
    "\n",
    "### Get actual outlier rows (using IQR method)\n",
    "Q1 = df_products['Price'].quantile(0.25)\n",
    "Q3 = df_products['Price'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df_products[(df_products['Price'] < lower_bound) | (df_products['Price'] > upper_bound)]\n",
    "\n",
    "print(\"Number of outliers:\", len(outliers))\n",
    "print(outliers[['Brand','Title','Price']].head(20))  # show first 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58087418",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# RFM-Like Analysis - Product \n",
    "# =========================\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#### Load your cleaned dataset (Products only) ===\n",
    "\n",
    "file_path = \"/Users/vini/Downloads/farfetch hackathon data/fashion_sales.xlsm\"\n",
    "df_products = pd.read_excel(file_path, sheet_name=\"Product_Master\")   # or CSV if you saved it\n",
    "\n",
    "\n",
    "print(\"‚úÖ Dataset loaded:\", df_products.shape)\n",
    "print(df_products.head())\n",
    "\n",
    "## ===  Recency calculation ===\n",
    "df_products['Recency'] = df_products['Details'].apply(lambda x: 1 if 'New Season' in str(x) else 0)\n",
    "\n",
    "## === Frequency calculation ===\n",
    "frequency = df_products.groupby('Brand')['Product_ID'].count().reset_index()\n",
    "frequency.rename(columns={'Product_ID': 'Frequency'}, inplace=True)\n",
    "\n",
    "## === Monetary calculation ===\n",
    "monetary = df_products.groupby('Brand')['Price'].mean().reset_index()\n",
    "monetary.rename(columns={'Price': 'Monetary'}, inplace=True)\n",
    "\n",
    "## ===  Merge RFM table ===\n",
    "rfm = df_products[['Brand', 'Recency']].drop_duplicates('Brand')\n",
    "rfm = rfm.merge(frequency, on=\"Brand\", how=\"left\")\n",
    "rfm = rfm.merge(monetary, on=\"Brand\", how=\"left\")\n",
    "\n",
    "print(\"\\nüìä RFM Table (first 10 rows):\")\n",
    "print(rfm.head(10))\n",
    "\n",
    "## === Save result ===\n",
    "rfm.to_csv(\"/Users/vini/Downloads/farfetch hackathon data/rfm_table.csv\", index=False)\n",
    "print(\"\\n‚úÖ RFM table saved as rfm_table.csv\")\n",
    "\n",
    "#%%\n",
    "# --------------------------\n",
    "#  Business Insights - Product\n",
    "# --------------------------\n",
    "\n",
    "### Premium vs Competitive pricing\n",
    "premium_brands = rfm[rfm['Monetary'] > 3000][['Brand', 'Monetary']].sort_values(by='Monetary', ascending=False)\n",
    "competitive_brands = rfm[rfm['Monetary'] < 1000][['Brand', 'Monetary']].sort_values(by='Monetary')\n",
    "\n",
    "print(\"\\nüíé Premium Brands (Avg > $3000):\")\n",
    "print(premium_brands.head(10))\n",
    "\n",
    "print(\"\\nüí≤ Competitive Brands (Avg < $1000):\")\n",
    "print(competitive_brands.head(10))\n",
    "\n",
    "### Most active in FW25 (Recency = 1 + High Frequency)\n",
    "active_fw25 = rfm[(rfm['Recency'] == 1) & (rfm['Frequency'] > 5)].sort_values(by='Frequency', ascending=False)\n",
    "\n",
    "print(\"\\nüî• Active FW25 Brands (High SKUs + New Season):\")\n",
    "print(active_fw25)\n",
    "\n",
    "\n",
    "### Risk of overstock: many SKUs but low pricing (< $1000)\n",
    "overstock_risk = rfm[(rfm['Frequency'] > 10) & (rfm['Monetary'] < 1000)]\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Overstock Risk Brands (High SKUs + Low Avg Price):\")\n",
    "print(overstock_risk)\n",
    "\n",
    "\n",
    "### Exclusive positioning: Few SKUs but very high price\n",
    "exclusive = rfm[(rfm['Frequency'] < 3) & (rfm['Monetary'] > 3000)]\n",
    "\n",
    "print(\"\\nüëë Exclusive Luxury Brands (Few SKUs + Very High Price):\")\n",
    "print(exclusive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c103c7",
   "metadata": {},
   "source": [
    "\n",
    "# =========================\n",
    "# Mock data EDA\n",
    "# =========================\n",
    "\n",
    "\n",
    "###  Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "file_path = r\"/Users/vini/Downloads/farfetch hackathon data/fashion_sales.xlsm\"\n",
    "\n",
    "### load product dataset\n",
    "df_mock = pd.read_excel(file_path, sheet_name=\"MockData\")\n",
    "\n",
    "### Data Overview\n",
    "print(\"‚úÖ Dataset Shape:\", df_mock.shape)\n",
    "\n",
    "print(\"\\nüìã Column Names:\")\n",
    "print(df_mock.columns.tolist())\n",
    "\n",
    "print(\"\\nüîç Missing Values per Column:\")\n",
    "print(df_mock.isnull().sum())\n",
    "\n",
    "print(\"\\nüåÄ Total Duplicate Rows:\", df_mock.duplicated().sum())\n",
    "\n",
    "\n",
    "### Basic Statistics\n",
    "print(\"\\nüìä Descriptive Statistics:\")\n",
    "print(df_mock.describe(include=\"all\").T)\n",
    "\n",
    "print(\"\\nüìä Median values (numeric only):\")\n",
    "print(df_mock.median(numeric_only=True))\n",
    "\n",
    "print(\"\\nüìä Range of numeric columns:\")\n",
    "for col in df_mock.select_dtypes(include='number').columns:\n",
    "    col_range = df_mock[col].max() - df_mock[col].min()\n",
    "    print(f\"{col}: {col_range}\")\n",
    "\n",
    "\n",
    "### Data Quality Checks\n",
    "if \"Price\" in df_mock.columns:\n",
    "    print(\"\\n‚ö†Ô∏è Price Values Check:\")\n",
    "    print(\"Min price:\", df_mock[\"Price\"].min(), \" | Max price:\", df_mock[\"Price\"].max())\n",
    "\n",
    "if \"Quantity\" in df_mock.columns:\n",
    "    print(\"\\n‚ö†Ô∏è Quantity Values Check:\")\n",
    "    print(\"Min quantity:\", df_mock[\"Quantity\"].min(), \" | Max quantity:\", df_mock[\"Quantity\"].max())\n",
    "\n",
    "\n",
    "### Outlier detection using IQR\n",
    "print(\"\\nüö® Outlier Check (IQR method):\")\n",
    "for col in df_mock.select_dtypes(include='number').columns:\n",
    "    Q1 = df_mock[col].quantile(0.25)\n",
    "    Q3 = df_mock[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df_mock[(df_mock[col] < lower_bound) | (df_mock[col] > upper_bound)][col]\n",
    "    print(f\"{col}: {len(outliers)} outliers\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b43015b",
   "metadata": {},
   "source": [
    "\n",
    "# =========================\n",
    "# Consumer Insight\n",
    "# =========================\n",
    "\n",
    "### --- Age Distribution ---\n",
    "\n",
    "    bins = [0, 25, 40, 55, 75, 100]\n",
    "    labels = [\"Gen Z (<25)\", \"Millennials (25-40)\", \"Gen X (40-55)\", \"Boomers (55-75)\", \"Older (75+)\"]\n",
    "    df_mock[\"AgeGroup\"] = pd.cut(df_mock[\"Age\"], bins=bins, labels=labels, right=False)\n",
    "    \n",
    "    age_dist = df_mock[\"AgeGroup\"].value_counts().sort_index()\n",
    "    print(\"\\nüìä Age Group Distribution:\\n\", age_dist)\n",
    "    \n",
    "    sns.countplot(data=df_mock, x=\"AgeGroup\", order=labels, palette=\"pastel\")\n",
    "    plt.title(\"Age Group Distribution\")\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.show()\n",
    "\n",
    "### --- Fashion Influence (Magazines vs Influencers) ---\n",
    "\n",
    "    magazines = df_mock[\"Fashion Magazines\"].value_counts().head(10)\n",
    "    influencers = df_mock[\"Fashion Influencers\"].value_counts().head(10)\n",
    "\n",
    "    print(\"\\nüì∞ Top Fashion Magazines:\\n\", magazines)\n",
    "    print(\"\\nüì± Top Fashion Influencers:\\n\", influencers)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14,5))\n",
    "    sns.barplot(x=magazines.values, y=magazines.index, ax=axes[0], palette=\"muted\")\n",
    "    axes[0].set_title(\"Top Fashion Magazines Influence\")\n",
    "    sns.barplot(x=influencers.values, y=influencers.index, ax=axes[1], palette=\"muted\")\n",
    "    axes[1].set_title(\"Top Fashion Influencers Influence\")\n",
    "    plt.show()\n",
    "\n",
    "### --- Purchase History ---\n",
    "\n",
    "    purchase_counts = df_mock[\"Purchase History\"].value_counts()\n",
    "    print(\"\\nüõçÔ∏è Purchase History:\\n\", purchase_counts)\n",
    "    \n",
    "    sns.countplot(data=df_mock, x=\"Purchase History\", palette=\"coolwarm\")\n",
    "    plt.title(\"Purchase History Distribution\")\n",
    "    plt.show()\n",
    "\n",
    "### --- Seasonal Demand ---\n",
    "\n",
    "    sns.countplot(data=df_mock, x=\"Season\", palette=\"Set2\", order=df_mock[\"Season\"].value_counts().index)\n",
    "    plt.title(\"Purchases by Season\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96dfa33",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# =========================\n",
    "#  Sentiment analysis\n",
    "# =========================\n",
    "\n",
    "\n",
    "### import library\n",
    "from textblob import TextBlob\n",
    "\n",
    "if \"Time Period Highest Purchase\" in df_mock.columns:\n",
    "    sns.countplot(data=df_mock, x=\"Time Period Highest Purchase\", palette=\"Set3\",\n",
    "                  order=df_mock[\"Time Period Highest Purchase\"].value_counts().index)\n",
    "    plt.title(\"Peak Time of Purchases\")\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.show()\n",
    "\n",
    "### --- Feedback Analysis (Sentiment from Reviews & Comments) ---\n",
    "def get_sentiment(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"Neutral\"\n",
    "    polarity = TextBlob(text).sentiment.polarity\n",
    "    if polarity > 0.1:\n",
    "        return \"Positive\"\n",
    "    elif polarity < -0.1:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "for col in [\"Customer Reviews\", \"Social Media Comments\"]:\n",
    "    if col in df_mock.columns:\n",
    "        df_mock[col + \"_Sentiment\"] = df_mock[col].apply(get_sentiment)\n",
    "        sentiment_counts = df_mock[col + \"_Sentiment\"].value_counts()\n",
    "        print(f\"\\nüí¨ Sentiment Distribution for {col}:\\n\", sentiment_counts)\n",
    "\n",
    "        sns.countplot(data=df_mock, x=col + \"_Sentiment\", order=[\"Positive\", \"Neutral\", \"Negative\"], palette=\"Spectral\")\n",
    "        plt.title(f\"Sentiment Analysis of {col}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c93779",
   "metadata": {},
   "source": [
    "# --------------------------\n",
    "# RFM Analysis - MOck\n",
    "# --------------------------\n",
    "\n",
    "### Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "### Load dataset\n",
    "file_path = r\"/Users/vini/Downloads/farfetch hackathon data/fashion_sales.xlsm\"\n",
    "df_mock = pd.read_excel(file_path, sheet_name=\"MockData\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 1. Handle Recency (Season)\n",
    "# --------------------------\n",
    "\n",
    "### Define a seasonal order (latest = lowest number = most recent)\n",
    "season_order = {\n",
    "    \"New Season\": 1,     # Most recent\n",
    "    \"Runway\": 2,         # Recent collection\n",
    "    \"Exclusive\": 3       # Older/limited availability\n",
    "}\n",
    "\n",
    "### Map to numeric recency score\n",
    "df_mock[\"Recency\"] = df_mock[\"Season\"].map(season_order)\n",
    "\n",
    "### Fill missing or unknown values with median\n",
    "df_mock[\"Recency\"].fillna(df_mock[\"Recency\"].median(), inplace=True)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "#  Frequency (Review Count)\n",
    "# --------------------------\n",
    "df_mock[\"Frequency\"] = df_mock[\"Review Count\"]\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "#  Monetary (Price)\n",
    "# --------------------------\n",
    "df_mock[\"Monetary\"] = df_mock[\"Price\"]\n",
    "\n",
    "# -----------------------------\n",
    "#  Prepare Recency, Frequency, Monetary\n",
    "# -----------------------------\n",
    "\n",
    "### prepare R, F, M...\n",
    "\n",
    "#### Recency -> Review Count (higher reviews = more recent/engaged)\n",
    "df_mock[\"Recency\"] = df_mock[\"Review Count\"].fillna(0)\n",
    "\n",
    "#### Frequency -> Count of appearances per Age (or customer proxy)\n",
    "df_mock[\"Frequency\"] = df_mock.groupby(\"Age\")[\"Title\"].transform(\"count\")\n",
    "\n",
    "#### Monetary -> Average Price per Age (or customer proxy)\n",
    "df_mock[\"Monetary\"] = df_mock.groupby(\"Age\")[\"Price\"].transform(\"mean\")\n",
    "\n",
    "# --------------------------\n",
    "# Assign RFM Scores\n",
    "# --------------------------\n",
    "\n",
    "#### Recency: higher review count = better (invert labels so higher ‚Üí 3)\n",
    "df_mock[\"R_score\"] = pd.qcut(df_mock[\"Recency\"].rank(method=\"first\"), q=3, labels=[1, 2, 3]).astype(int)\n",
    "\n",
    "#### Frequency: higher = better\n",
    "df_mock[\"F_score\"] = pd.qcut(df_mock[\"Frequency\"].rank(method=\"first\"), q=3, labels=[1, 2, 3]).astype(int)\n",
    "\n",
    "#### Monetary: higher = better\n",
    "df_mock[\"M_score\"] = pd.qcut(df_mock[\"Monetary\"].rank(method=\"first\"), q=3, labels=[1, 2, 3]).astype(int)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "#  Final RFM Segment\n",
    "# --------------------------\n",
    "\n",
    "df_mock[\"RFM_Segment\"] = df_mock[\"R_score\"].astype(str) + df_mock[\"F_score\"].astype(str) + df_mock[\"M_score\"].astype(str)\n",
    "df_mock[\"RFM_Score\"] = df_mock[[\"R_score\", \"F_score\", \"M_score\"]].sum(axis=1)\n",
    "\n",
    "# --------------------------\n",
    "#  Define Customer Levels\n",
    "# --------------------------\n",
    "\n",
    "def segment_me(df):\n",
    "    if df[\"RFM_Score\"] >= 8:\n",
    "        return \"High-Value\"\n",
    "    elif df[\"RFM_Score\"] >= 5:\n",
    "        return \"Mid-Value\"\n",
    "    else:\n",
    "        return \"Low-Value\"\n",
    "\n",
    "df_mock[\"Customer_Segment\"] = df_mock.apply(segment_me, axis=1)\n",
    "\n",
    "# --------------------------\n",
    "#  Output\n",
    "# --------------------------\n",
    "\n",
    "rfm_summary = df_mock.groupby(\"Customer_Segment\")[[\"Recency\", \"Frequency\", \"Monetary\"]].mean().round(2)\n",
    "\n",
    "print(\"‚úÖ RFM Segmentation Done!\")\n",
    "print(rfm_summary.head())\n",
    "\n",
    "### Save cleaned RFM file\n",
    "output_path = r\"/Users/vini/Downloads/farfetch hackathon data/RFM_Output.csv\"\n",
    "df_mock.to_csv(output_path, index=False)\n",
    "print(f\"üìÇ File saved to: {output_path}\")\n",
    "\n",
    "\n",
    "df_mock[\"Monetary\"] = df_mock.groupby(\"Age\")[\"Price\"].transform(\"mean\")\n",
    "\n",
    "# --------------------------\n",
    "#  RFM Scoring\n",
    "# --------------------------\n",
    "df_mock[\"R_score\"] = pd.qcut(df_mock[\"Recency\"].rank(method=\"first\"), q=3, labels=[1, 2, 3]).astype(int)\n",
    "df_mock[\"F_score\"] = pd.qcut(df_mock[\"Frequency\"].rank(method=\"first\"), q=3, labels=[1, 2, 3]).astype(int)\n",
    "df_mock[\"M_score\"] = pd.qcut(df_mock[\"Monetary\"].rank(method=\"first\"), q=3, labels=[1, 2, 3]).astype(int)\n",
    "\n",
    "df_mock[\"RFM_Segment\"] = df_mock[\"R_score\"].astype(str) + df_mock[\"F_score\"].astype(str) + df_mock[\"M_score\"].astype(str)\n",
    "df_mock[\"RFM_Score\"] = df_mock[[\"R_score\", \"F_score\", \"M_score\"]].sum(axis=1)\n",
    "\n",
    "def segment_me(df):\n",
    "    if df[\"RFM_Score\"] >= 8:\n",
    "        return \"High-Value\"\n",
    "    elif df[\"RFM_Score\"] >= 5:\n",
    "        return \"Mid-Value\"\n",
    "    else:\n",
    "        return \"Low-Value\"\n",
    "\n",
    "df_mock[\"Customer_Segment\"] = df_mock.apply(segment_me, axis=1)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Summary\n",
    "# --------------------------\n",
    "rfm_summary = df_mock.groupby(\"Customer_Segment\")[[\"Recency\", \"Frequency\", \"Monetary\"]].mean().round(2)\n",
    "print(\"‚úÖ RFM Segmentation Done!\")\n",
    "print(rfm_summary)\n",
    "\n",
    "# --------------------------\n",
    "#  Visualizations\n",
    "# --------------------------\n",
    "\n",
    "### Bar plot of customer segments\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=df_mock, x=\"Customer_Segment\", palette=\"viridis\", order=[\"High-Value\", \"Mid-Value\", \"Low-Value\"])\n",
    "plt.title(\"Customer Segment Distribution\")\n",
    "plt.xlabel(\"Segment\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "### Boxplot for Recency, Frequency, Monetary by Segment\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18,5))\n",
    "\n",
    "sns.boxplot(data=df_mock, x=\"Customer_Segment\", y=\"Recency\", order=[\"High-Value\", \"Mid-Value\", \"Low-Value\"], palette=\"viridis\", ax=axes[0])\n",
    "axes[0].set_title(\"Recency by Segment\")\n",
    "\n",
    "sns.boxplot(data=df_mock, x=\"Customer_Segment\", y=\"Frequency\", order=[\"High-Value\", \"Mid-Value\", \"Low-Value\"], palette=\"viridis\", ax=axes[1])\n",
    "axes[1].set_title(\"Frequency by Segment\")\n",
    "\n",
    "sns.boxplot(data=df_mock, x=\"Customer_Segment\", y=\"Monetary\", order=[\"High-Value\", \"Mid-Value\", \"Low-Value\"], palette=\"viridis\", ax=axes[2])\n",
    "axes[2].set_title(\"Monetary by Segment\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "### Heatmap for average RFM values\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(rfm_summary.T, annot=True, cmap=\"YlGnBu\", cbar=False)\n",
    "plt.title(\"Average RFM Values by Customer Segment\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60483e74",
   "metadata": {},
   "source": [
    "# -----------------------------------\n",
    "# Churn Analysis\n",
    "# -----------------------------------\n",
    "\n",
    "\n",
    "### -----------------------------------\n",
    "### Define Churn vs Active\n",
    "### -----------------------------------\n",
    "\n",
    "#### Define thresholds\n",
    "recency_threshold = df_mock[\"Recency\"].median()\n",
    "freq_threshold = df_mock[\"Frequency\"].median()\n",
    "\n",
    "def churn_flag(row):\n",
    "    if row[\"Recency\"] < recency_threshold and row[\"Frequency\"] < freq_threshold:\n",
    "        return \"Churn\"\n",
    "    else:\n",
    "        return \"Active\"\n",
    "\n",
    "df_mock[\"Churn_Status\"] = df_mock.apply(churn_flag, axis=1)\n",
    "\n",
    "\n",
    "### Churn Rate\n",
    "\n",
    "churn_rate = df_mock[\"Churn_Status\"].value_counts(normalize=True) * 100\n",
    "print(\"Churn Rate (%)\\n\", churn_rate)\n",
    "\n",
    "\n",
    "## Compare Churn vs Active\n",
    "\n",
    "#### By Age Group\n",
    "df_mock[\"AgeGroup\"] = pd.cut(\n",
    "    df_mock[\"Age\"],\n",
    "    bins=[0, 24, 40, 55, 75, 100],\n",
    "    labels=[\"Gen Z (<25)\", \"Millennials (25-40)\", \"Gen X (40-55)\", \"Boomers (55-75)\", \"Older (75+)\"]\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(data=df_mock, x=\"AgeGroup\", hue=\"Churn_Status\", palette=\"Set2\")\n",
    "plt.title(\"Churn vs Active by Age Group\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "#### By Category (proxy for gender/region)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(data=df_mock, x=\"Category\", hue=\"Churn_Status\", palette=\"Set1\",\n",
    "              order=df_mock[\"Category\"].value_counts().index)\n",
    "plt.title(\"Churn vs Active by Category\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "#### By Order Value\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.boxplot(data=df_mock, x=\"Churn_Status\", y=\"Monetary\", palette=\"coolwarm\")\n",
    "plt.title(\"Churn vs Active by Order Value (AOV)\")\n",
    "plt.show()\n",
    "\n",
    "#### By Rating\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.boxplot(data=df_mock, x=\"Churn_Status\", y=\"Rating\", palette=\"viridis\")\n",
    "plt.title(\"Churn vs Active by Product Ratings\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d99a45",
   "metadata": {},
   "source": [
    "# -----------------------------\n",
    "# PRODUCT INSIGHTS - Sales\n",
    "# -----------------------------\n",
    "\n",
    "print(\"\\nüîé Product Insights\")\n",
    "\n",
    "### Best-selling product categories\n",
    "if \"Category\" in df_mock.columns:\n",
    "    best_categories = (\n",
    "        df_mock.groupby(\"Category\")[\"Frequency\"].sum()\n",
    "        .sort_values(ascending=False)\n",
    "        .head(10)\n",
    "    )\n",
    "    \n",
    "    \n",
    " print(\"\\nüìå Top Categories by Sales:\\n\", best_categories)\n",
    "    \n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.barplot(x=best_categories.values, y=best_categories.index, palette=\"mako\")\n",
    "    plt.title(\"Top Product Categories by Sales\")\n",
    "    plt.xlabel(\"Total Purchases (Frequency)\")\n",
    "    plt.ylabel(\"Category\")\n",
    "    plt.show()\n",
    "\n",
    "### Price vs Popularity (scatter plot)\n",
    "if \"Price\" in df_mock.columns:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.scatterplot(data=df_mock, x=\"Price\", y=\"Frequency\", alpha=0.6)\n",
    "    plt.title(\"Price vs Popularity (Frequency)\")\n",
    "    plt.xlabel(\"Price\")\n",
    "    plt.ylabel(\"Popularity (Purchases/Reviews)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "### Top brands/products by revenue\n",
    "if \"Brand\" in df_mock.columns and \"Monetary\" in df_mock.columns:\n",
    "    top_brands = (\n",
    "        df_mock.groupby(\"Brand\")[\"Monetary\"].sum()\n",
    "        .sort_values(ascending=False)\n",
    "        .head(10)\n",
    "    )\n",
    "    print(\"\\nüíé Top Brands by Revenue:\\n\", top_brands)\n",
    "    \n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.barplot(x=top_brands.values, y=top_brands.index, palette=\"viridis\")\n",
    "    plt.title(\"Top 10 Brands by Revenue\")\n",
    "    plt.xlabel(\"Revenue\")\n",
    "    plt.ylabel(\"Brand\")\n",
    "    plt.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae7099",
   "metadata": {},
   "source": [
    "#%%\n",
    "# -----------------------------\n",
    "# PRODUCT CLUSTERING - Mock\n",
    "# -----------------------------\n",
    "\n",
    "#### clustering_only.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "\n",
    "df_mock[\"LogPrice\"] = np.log1p(df_mock[\"Price\"])  # log transform for skew\n",
    "df_mock[\"Review_Count\"] = pd.to_numeric(df_mock[\"Review Count\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "### Encode feedback sentiment\n",
    "df_mock[\"Feedback_Score\"] = df_mock[\"feedback\"].astype(str).str.lower().map({\n",
    "    \"positive\": 2,\n",
    "    \"neutral\": 1,\n",
    "    \"negative\": 0\n",
    "})\n",
    "df_mock[\"Feedback_Score\"] = df_mock[\"Feedback_Score\"].fillna(1)  # default neutral\n",
    "\n",
    "### Select features for clustering\n",
    "features = [\"LogPrice\", \"Review_Count\", \"Age\", \"Feedback_Score\"]\n",
    "df_cluster = df_mock[features].dropna().copy()\n",
    "\n",
    "\n",
    "## Scaling & KMeans\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_cluster)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "df_cluster[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "\n",
    "## Visualization\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=df_cluster,\n",
    "    x=\"LogPrice\",\n",
    "    y=\"Review_Count\",\n",
    "    hue=\"Cluster\",\n",
    "    palette=\"Set2\",\n",
    "    s=60\n",
    ")\n",
    "plt.title(\"Product Clusters\")\n",
    "plt.xlabel(\"Log Price\")\n",
    "plt.ylabel(\"Review Count\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Clustering done. Cluster counts:\")\n",
    "print(df_cluster[\"Cluster\"].value_counts())\n",
    "\n",
    "## Cluster profiling\n",
    "cluster_profile = df_cluster.groupby(\"Cluster\").mean()\n",
    "print(\"\\nüìä Cluster Profiles:\\n\", cluster_profile)\n",
    "\n",
    "sse = []\n",
    "for k in range(2, 8):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(range(2, 8), sse, marker=\"o\")\n",
    "plt.title(\"Elbow Method for Optimal k\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"SSE (Inertia)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Business Insight - Sales\n",
    "# ----------------------------- \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "\n",
    "### Select features for clustering (adjust these to what you actually have)\n",
    "features = [\"Recency\", \"Frequency\", \"Monetary\"]  # Example RFM columns\n",
    "X = df_mock[features].copy()\n",
    "\n",
    "### Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "### Apply KMeans\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "df_mock[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "### Now df_mock DEFINITELY has Cluster column\n",
    "print(df_mock.head())\n",
    "\n",
    "### Group by Cluster - Revenue (if you have Price column)\n",
    "if \"Price\" in df_mock.columns:\n",
    "    cluster_revenue = df_mock.groupby(\"Cluster\")[\"Price\"].sum().sort_values(ascending=False)\n",
    "    print(\"\\nRevenue by Cluster:\\n\", cluster_revenue)\n",
    "\n",
    "### Group by Cluster - Size of each cluster\n",
    "cluster_size = df_mock[\"Cluster\"].value_counts().sort_index()\n",
    "print(\"\\nCluster Sizes:\\n\", cluster_size)\n",
    "\n",
    "### Export for Tableau\n",
    "output_path = \"/Users/vini/Downloads/farfetch hackathon data/clustered_output.xlsx\"\n",
    "df_mock.to_excel(output_path, index=False)\n",
    "print(f\"\\nClustered data saved to: {output_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
